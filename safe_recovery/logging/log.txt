Logging to logging/
Using MAIN agent with the following configuration:
dict_items([('name', 'main'), ('env', <baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f99db8b3358>), ('config', Munch({'render': False, 'network': 'mlp', 'replay_memory_size': 1000000, 'gamma': 0.99, 'tau': 0.01, 'reward_scale': 1.0, 'batch_size': 64, 'critic_l2_reg': 0.01, 'actor_lr': 0.0001, 'critic_lr': 0.001, 'normalize_returns': False, 'normalize_observations': False, 'popart': False, 'clip_norm': None, 'num_epochs': 500, 'num_cycles_per_epoch': 1, 'num_train_steps_per_cycle': 50, 'num_rollout_steps': 5000, 'param_noise_adaption_interval': 50, 'save_freq': 10, 'logtostdout_freq': 100, 'do_eval': False, 'eval_freq': 10, 'random_seed': 7, 'noise_type': 'adaptive-param', 'noise_std': 0.2, 'max_checkpoints_to_keep': 100})), ('nb_actions', 3), ('_training', True), ('Actor', <baselines.ddpg.models.Actor object at 0x7f99db88c0f0>), ('Critic', <baselines.ddpg.models.Critic object at 0x7f99db88c048>), ('Memory', <baselines.ddpg.memory.Memory object at 0x7f99db88c1d0>), ('noise', <model_base.ExplorationNoise object at 0x7f99db88c160>), ('agent', <baselines.ddpg.ddpg_learner.DDPG object at 0x7f99db88c2b0>)])
Using RECOVERY agent with the following configuration:
dict_items([('name', 'recovery'), ('env', <baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f99db8b3358>), ('config', Munch({'render': False, 'network': 'mlp', 'replay_memory_size': 1000000, 'gamma': 0.99, 'tau': 0.01, 'reward_scale': 1.0, 'batch_size': 64, 'critic_l2_reg': 0.01, 'actor_lr': 0.0001, 'critic_lr': 0.001, 'normalize_returns': False, 'normalize_observations': False, 'popart': False, 'clip_norm': None, 'num_epochs': 500, 'num_cycles_per_epoch': 1, 'num_train_steps_per_cycle': 50, 'num_rollout_steps': 5000, 'param_noise_adaption_interval': 50, 'save_freq': 10, 'logtostdout_freq': 100, 'do_eval': False, 'eval_freq': 10, 'random_seed': 7, 'noise_type': 'adaptive-param', 'noise_std': 0.2, 'max_checkpoints_to_keep': 100})), ('nb_actions', 3), ('_training', True), ('Actor', <baselines.ddpg.models.Actor object at 0x7f99d0b146d8>), ('Critic', <baselines.ddpg.models.Critic object at 0x7f99d0b58e80>), ('Memory', <baselines.ddpg.memory.Memory object at 0x7f99d0b2e080>), ('noise', <model_base.ExplorationNoise object at 0x7f99d0b24a90>), ('agent', <baselines.ddpg.ddpg_learner.DDPG object at 0x7f99d0b24940>)])
--------------------------------------
| param_noise_stddev      | 0.198    |
| reference_action_mean   | -0.0228  |
| reference_action_std    | 0.0915   |
| reference_actor_Q_mean  | 0.0292   |
| reference_actor_Q_std   | 0.000398 |
| reference_perturbed_... | 0.591    |
| reference_Q_mean        | 0.0257   |
| reference_Q_std         | 0.00144  |
| rollout/run_avg_eps_... | nan      |
| total/epoch_duration    | 17       |
| total/num_epochs        | 1        |
| train/main_actor_loss   | -0.0303  |
| train/main_critic_loss  | 0.774    |
| train/recovery_actor... | nan      |
| train/recovery_criti... | nan      |
--------------------------------------

Episode reward: -39.72040557861328
--------------------------------------
| param_noise_stddev      | 0.196    |
| reference_action_mean   | -0.251   |
| reference_action_std    | 0.129    |
| reference_actor_Q_mean  | 0.028    |
| reference_actor_Q_std   | 0.0013   |
| reference_perturbed_... | 0.442    |
| reference_Q_mean        | 0.0161   |
| reference_Q_std         | 0.00357  |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 18.4     |
| total/num_epochs        | 2        |
| train/main_actor_loss   | -0.0191  |
| train/main_critic_loss  | 0.487    |
| train/recovery_actor... | nan      |
| train/recovery_criti... | nan      |
--------------------------------------

--------------------------------------
| param_noise_stddev      | 0.194    |
| reference_action_mean   | -0.556   |
| reference_action_std    | 0.146    |
| reference_actor_Q_mean  | 0.0311   |
| reference_actor_Q_std   | 0.000818 |
| reference_perturbed_... | 0.442    |
| reference_Q_mean        | 0.015    |
| reference_Q_std         | 0.00282  |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 17.1     |
| total/num_epochs        | 3        |
| train/main_actor_loss   | -0.0287  |
| train/main_critic_loss  | 0.309    |
| train/recovery_actor... | nan      |
| train/recovery_criti... | nan      |
--------------------------------------

Episode reward: -22.852500915527344
--------------------------------------
| param_noise_stddev      | 0.192    |
| reference_action_mean   | -0.693   |
| reference_action_std    | 0.171    |
| reference_actor_Q_mean  | 0.0342   |
| reference_actor_Q_std   | 0.00071  |
| reference_perturbed_... | 0.451    |
| reference_Q_mean        | 0.0213   |
| reference_Q_std         | 0.00213  |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 18.4     |
| total/num_epochs        | 4        |
| train/main_actor_loss   | -0.0321  |
| train/main_critic_loss  | 0.197    |
| train/recovery_actor... | nan      |
| train/recovery_criti... | nan      |
--------------------------------------

--------------------------------------
| param_noise_stddev      | 0.194    |
| reference_action_mean   | -0.777   |
| reference_action_std    | 0.0742   |
| reference_actor_Q_mean  | 0.0488   |
| reference_actor_Q_std   | 0.00174  |
| reference_perturbed_... | 0.451    |
| reference_Q_mean        | 0.0348   |
| reference_Q_std         | 0.0037   |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 16.9     |
| total/num_epochs        | 5        |
| train/main_actor_loss   | -0.044   |
| train/main_critic_loss  | 0.126    |
| train/recovery_actor... | nan      |
| train/recovery_criti... | nan      |
--------------------------------------

Episode reward: -3643.604736328125
--------------------------------------
| param_noise_stddev      | 0.192    |
| reference_action_mean   | -0.691   |
| reference_action_std    | 0.319    |
| reference_actor_Q_mean  | 0.198    |
| reference_actor_Q_std   | 0.0111   |
| reference_perturbed_... | 0.25     |
| reference_Q_mean        | 0.00694  |
| reference_Q_std         | 0.0244   |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 18.7     |
| total/num_epochs        | 6        |
| train/main_actor_loss   | -0.13    |
| train/main_critic_loss  | 0.501    |
| train/recovery_actor... | 0.514    |
| train/recovery_criti... | 1.09     |
--------------------------------------

--------------------------------------
| param_noise_stddev      | 0.19     |
| reference_action_mean   | -0.354   |
| reference_action_std    | 0.708    |
| reference_actor_Q_mean  | 0.341    |
| reference_actor_Q_std   | 0.0434   |
| reference_perturbed_... | 0.25     |
| reference_Q_mean        | 0.0902   |
| reference_Q_std         | 0.0648   |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 16.7     |
| total/num_epochs        | 7        |
| train/main_actor_loss   | -0.352   |
| train/main_critic_loss  | 0.426    |
| train/recovery_actor... | 0.664    |
| train/recovery_criti... | 0.584    |
--------------------------------------

Episode reward: -22.679954528808594
--------------------------------------
| param_noise_stddev      | 0.188    |
| reference_action_mean   | -0.00922 |
| reference_action_std    | 0.751    |
| reference_actor_Q_mean  | 0.562    |
| reference_actor_Q_std   | 0.0263   |
| reference_perturbed_... | 0.894    |
| reference_Q_mean        | 0.0624   |
| reference_Q_std         | 0.0617   |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 18.6     |
| total/num_epochs        | 8        |
| train/main_actor_loss   | -0.524   |
| train/main_critic_loss  | 0.24     |
| train/recovery_actor... | 0.535    |
| train/recovery_criti... | 0.397    |
--------------------------------------

--------------------------------------
| param_noise_stddev      | 0.187    |
| reference_action_mean   | 0.22     |
| reference_action_std    | 0.86     |
| reference_actor_Q_mean  | 0.88     |
| reference_actor_Q_std   | 0.0436   |
| reference_perturbed_... | 0.894    |
| reference_Q_mean        | 0.12     |
| reference_Q_std         | 0.0711   |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 15.8     |
| total/num_epochs        | 9        |
| train/main_actor_loss   | -1.25    |
| train/main_critic_loss  | 0.43     |
| train/recovery_actor... | 0.306    |
| train/recovery_criti... | 0.375    |
--------------------------------------

Episode reward: -5999.349609375
--------------------------------------
| param_noise_stddev      | 0.188    |
| reference_action_mean   | 0.264    |
| reference_action_std    | 0.89     |
| reference_actor_Q_mean  | 0.561    |
| reference_actor_Q_std   | 0.0431   |
| reference_perturbed_... | 0.417    |
| reference_Q_mean        | 0.169    |
| reference_Q_std         | 0.0442   |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 17.7     |
| total/num_epochs        | 10       |
| train/main_actor_loss   | -0.88    |
| train/main_critic_loss  | 0.254    |
| train/recovery_actor... | 0.396    |
| train/recovery_criti... | 0.2      |
--------------------------------------

--------------------------------------
| param_noise_stddev      | 0.19     |
| reference_action_mean   | 0.271    |
| reference_action_std    | 0.895    |
| reference_actor_Q_mean  | 0.27     |
| reference_actor_Q_std   | 0.0521   |
| reference_perturbed_... | 0.417    |
| reference_Q_mean        | 0.107    |
| reference_Q_std         | 0.0499   |
| rollout/run_avg_eps_... | 5e+03    |
| total/epoch_duration    | 16.7     |
| total/num_epochs        | 11       |
| train/main_actor_loss   | -0.373   |
| train/main_critic_loss  | 0.411    |
| train/recovery_actor... | 0.329    |
| train/recovery_criti... | 0.14     |
--------------------------------------

